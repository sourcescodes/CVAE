{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c675e477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:35:05.207332Z",
     "start_time": "2023-04-10T08:35:05.111586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"import tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.neighbors import NearestNeighbors\";\n",
       "                var nbb_formatted_code = \"import tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.neighbors import NearestNeighbors\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e3fea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:35:10.538855Z",
     "start_time": "2023-04-10T08:35:10.467076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"class CVAE(tf.keras.Model):\\n    def __init__(self, input_dim, latent_dim, hidden_dim):\\n        super(CVAE, self).__init__()\\n        \\n        self.input_dim = input_dim\\n        self.latent_dim = latent_dim\\n        self.hidden_dim = hidden_dim\\n        \\n        self.encoder = tf.keras.Sequential([\\n            tf.keras.layers.InputLayer(input_shape=(self.input_dim,)),\\n            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\\n            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\\n            tf.keras.layers.Dense(self.latent_dim * 2)\\n        ])\\n        \\n        self.decoder = tf.keras.Sequential([\\n            tf.keras.layers.InputLayer(input_shape=(self.latent_dim,)),\\n            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\\n            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\\n            tf.keras.layers.Dense(self.input_dim)\\n        ])\\n        \\n        self.coupled_encoder = tf.keras.Sequential([\\n            tf.keras.layers.InputLayer(input_shape=(self.input_dim + self.input_dim,)),\\n            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\\n            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\\n            tf.keras.layers.Dense(self.latent_dim * 2)\\n        ])\\n    \\n    def encode(self, x):\\n        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\\n        return mean, logvar\\n    \\n    def reparameterize(self, mean, logvar):\\n        eps = tf.random.normal(shape=mean.shape)\\n        return eps * tf.exp(logvar * 0.5) + mean\\n    \\n    def decode(self, z):\\n        return self.decoder(z)\\n    \\n    def coupled_encode(self, x):\\n        return self.coupled_encoder(x)\\n    \\n    def call(self, x):\\n        x1, x2 = x\\n        \\n        # encode scRNA-seq data and sample from the distribution\\n        mean1, logvar1 = self.encode(x1)\\n        z1 = self.reparameterize(mean1, logvar1)\\n        \\n        # encode scATAC-seq data and sample from the distribution\\n        mean2, logvar2 = self.encode(x2)\\n        z2 = self.reparameterize(mean2, logvar2)\\n        \\n        # concatenate the two latent variables\\n        z = tf.concat([z1, z2], axis=1)\\n        \\n        # decode from the concatenated latent variable\\n        x_recon = self.decode(z)\\n        \\n        return x_recon, mean1, logvar1, mean2, logvar2\";\n",
       "                var nbb_formatted_code = \"class CVAE(tf.keras.Model):\\n    def __init__(self, input_dim, latent_dim, hidden_dim):\\n        super(CVAE, self).__init__()\\n\\n        self.input_dim = input_dim\\n        self.latent_dim = latent_dim\\n        self.hidden_dim = hidden_dim\\n\\n        self.encoder = tf.keras.Sequential(\\n            [\\n                tf.keras.layers.InputLayer(input_shape=(self.input_dim,)),\\n                tf.keras.layers.Dense(self.hidden_dim, activation=\\\"relu\\\"),\\n                tf.keras.layers.Dense(self.hidden_dim, activation=\\\"relu\\\"),\\n                tf.keras.layers.Dense(self.latent_dim * 2),\\n            ]\\n        )\\n\\n        self.decoder = tf.keras.Sequential(\\n            [\\n                tf.keras.layers.InputLayer(input_shape=(self.latent_dim,)),\\n                tf.keras.layers.Dense(self.hidden_dim, activation=\\\"relu\\\"),\\n                tf.keras.layers.Dense(self.hidden_dim, activation=\\\"relu\\\"),\\n                tf.keras.layers.Dense(self.input_dim),\\n            ]\\n        )\\n\\n        self.coupled_encoder = tf.keras.Sequential(\\n            [\\n                tf.keras.layers.InputLayer(\\n                    input_shape=(self.input_dim + self.input_dim,)\\n                ),\\n                tf.keras.layers.Dense(self.hidden_dim, activation=\\\"relu\\\"),\\n                tf.keras.layers.Dense(self.hidden_dim, activation=\\\"relu\\\"),\\n                tf.keras.layers.Dense(self.latent_dim * 2),\\n            ]\\n        )\\n\\n    def encode(self, x):\\n        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\\n        return mean, logvar\\n\\n    def reparameterize(self, mean, logvar):\\n        eps = tf.random.normal(shape=mean.shape)\\n        return eps * tf.exp(logvar * 0.5) + mean\\n\\n    def decode(self, z):\\n        return self.decoder(z)\\n\\n    def coupled_encode(self, x):\\n        return self.coupled_encoder(x)\\n\\n    def call(self, x):\\n        x1, x2 = x\\n\\n        # encode scRNA-seq data and sample from the distribution\\n        mean1, logvar1 = self.encode(x1)\\n        z1 = self.reparameterize(mean1, logvar1)\\n\\n        # encode scATAC-seq data and sample from the distribution\\n        mean2, logvar2 = self.encode(x2)\\n        z2 = self.reparameterize(mean2, logvar2)\\n\\n        # concatenate the two latent variables\\n        z = tf.concat([z1, z2], axis=1)\\n\\n        # decode from the concatenated latent variable\\n        x_recon = self.decode(z)\\n\\n        return x_recon, mean1, logvar1, mean2, logvar2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.input_dim,)),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.latent_dim * 2)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.latent_dim,)),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.input_dim)\n",
    "        ])\n",
    "        \n",
    "        self.coupled_encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.input_dim + self.input_dim,)),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.hidden_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.latent_dim * 2)\n",
    "        ])\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def coupled_encode(self, x):\n",
    "        return self.coupled_encoder(x)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x1, x2 = x\n",
    "        \n",
    "        # encode scRNA-seq data and sample from the distribution\n",
    "        mean1, logvar1 = self.encode(x1)\n",
    "        z1 = self.reparameterize(mean1, logvar1)\n",
    "        \n",
    "        # encode scATAC-seq data and sample from the distribution\n",
    "        mean2, logvar2 = self.encode(x2)\n",
    "        z2 = self.reparameterize(mean2, logvar2)\n",
    "        \n",
    "        # concatenate the two latent variables\n",
    "        z = tf.concat([z1, z2], axis=1)\n",
    "        \n",
    "        # decode from the concatenated latent variable\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        return x_recon, mean1, logvar1, mean2, logvar2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9776a48c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:35:12.815197Z",
     "start_time": "2023-04-10T08:35:12.769227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"def compute_loss(model, x, k=5):\\n    x1, x2 = x\\n    x_recon, mean1, logvar1, mean2, logvar2 = model([x1, x2])\\n    \\n    # compute reconstruction loss\\n    recon_loss = tf.reduce_mean(tf.square(x_recon - x1))\\n    \\n    # compute KL divergence loss\\n    kl_loss1 = -0.5 * tf.reduce_mean(1 + logvar1 - tf.square(mean1) - tf.exp(logvar1))\\n    kl_loss2 = -0.5 * tf.reduce_mean(1 + logvar2 - tf.square(mean2) - tf.exp(logvar2))\\n    \\n    # compute matching loss\\n    z1 = model.encode(x1)[0]\\n    z2 = model.encode(x2)[0]\\n    \\n    # compute k-nearest neighbors for scRNA-seq data\\n    nbrs1 = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(z1)\\n    distances1, indices1 = nbrs1.kneighbors(z2)\\n    \\n    # compute k-nearest neighbors for scATAC-seq data\\n    nbrs2 = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(z2)\\n    distances2, indices2 = nbrs2.kneighbors(z1)\\n    \\n    # compute optimal matching using Hungarian algorithm\\n    cost_matrix = np.zeros((x1.shape[0], x2.shape[0]))\\n    for i, j in zip(indices1.flatten(), range(indices1.shape[0])):\\n        cost_matrix[i, j] = distances1.flatten()[j]\\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\\n    \\n    # compute matching loss\\n    match_loss = np.mean(cost_matrix[row_ind, col_ind])\\n    \\n    return recon_loss, kl_loss1, kl_loss2, match_loss\";\n",
       "                var nbb_formatted_code = \"def compute_loss(model, x, k=5):\\n    x1, x2 = x\\n    x_recon, mean1, logvar1, mean2, logvar2 = model([x1, x2])\\n\\n    # compute reconstruction loss\\n    recon_loss = tf.reduce_mean(tf.square(x_recon - x1))\\n\\n    # compute KL divergence loss\\n    kl_loss1 = -0.5 * tf.reduce_mean(1 + logvar1 - tf.square(mean1) - tf.exp(logvar1))\\n    kl_loss2 = -0.5 * tf.reduce_mean(1 + logvar2 - tf.square(mean2) - tf.exp(logvar2))\\n\\n    # compute matching loss\\n    z1 = model.encode(x1)[0]\\n    z2 = model.encode(x2)[0]\\n\\n    # compute k-nearest neighbors for scRNA-seq data\\n    nbrs1 = NearestNeighbors(n_neighbors=k, algorithm=\\\"ball_tree\\\").fit(z1)\\n    distances1, indices1 = nbrs1.kneighbors(z2)\\n\\n    # compute k-nearest neighbors for scATAC-seq data\\n    nbrs2 = NearestNeighbors(n_neighbors=k, algorithm=\\\"ball_tree\\\").fit(z2)\\n    distances2, indices2 = nbrs2.kneighbors(z1)\\n\\n    # compute optimal matching using Hungarian algorithm\\n    cost_matrix = np.zeros((x1.shape[0], x2.shape[0]))\\n    for i, j in zip(indices1.flatten(), range(indices1.shape[0])):\\n        cost_matrix[i, j] = distances1.flatten()[j]\\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\\n\\n    # compute matching loss\\n    match_loss = np.mean(cost_matrix[row_ind, col_ind])\\n\\n    return recon_loss, kl_loss1, kl_loss2, match_loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_loss(model, x, k=5):\n",
    "    x1, x2 = x\n",
    "    x_recon, mean1, logvar1, mean2, logvar2 = model([x1, x2])\n",
    "    \n",
    "    # compute reconstruction loss\n",
    "    recon_loss = tf.reduce_mean(tf.square(x_recon - x1))\n",
    "    \n",
    "    # compute KL divergence loss\n",
    "    kl_loss1 = -0.5 * tf.reduce_mean(1 + logvar1 - tf.square(mean1) - tf.exp(logvar1))\n",
    "    kl_loss2 = -0.5 * tf.reduce_mean(1 + logvar2 - tf.square(mean2) - tf.exp(logvar2))\n",
    "    \n",
    "    # compute matching loss\n",
    "    z1 = model.encode(x1)[0]\n",
    "    z2 = model.encode(x2)[0]\n",
    "    \n",
    "    # compute k-nearest neighbors for scRNA-seq data\n",
    "    nbrs1 = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(z1)\n",
    "    distances1, indices1 = nbrs1.kneighbors(z2)\n",
    "    \n",
    "    # compute k-nearest neighbors for scATAC-seq data\n",
    "    nbrs2 = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(z2)\n",
    "    distances2, indices2 = nbrs2.kneighbors(z1)\n",
    "    \n",
    "    # compute optimal matching using Hungarian algorithm\n",
    "    cost_matrix = np.zeros((x1.shape[0], x2.shape[0]))\n",
    "    for i, j in zip(indices1.flatten(), range(indices1.shape[0])):\n",
    "        cost_matrix[i, j] = distances1.flatten()[j]\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    # compute matching loss\n",
    "    match_loss = np.mean(cost_matrix[row_ind, col_ind])\n",
    "    \n",
    "    return recon_loss, kl_loss1, kl_loss2, match_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bea3552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:39:05.178293Z",
     "start_time": "2023-04-10T08:39:05.125426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# define optimizer\\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\\n\\n# define early stopping\\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\\n\\n# train the model\\ndef train(model, x_train, x_val, epochs, batch_size):\\n    train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(len(x_train)).batch(batch_size)\\n    val_dataset = tf.data.Dataset.from_tensor_slices(x_val).batch(batch_size)\\n    \\n    train_loss_results = []\\n    val_loss_results = []\\n    \\n    for epoch in range(epochs):\\n        train_loss_avg = tf.keras.metrics.Mean()\\n        val_loss_avg = tf.keras.metrics.Mean()\\n        \\n        # train the model\\n        for x in train_dataset:\\n            with tf.GradientTape() as tape:\\n                recon_loss, kl_loss1, kl_loss2, match_loss = compute_loss(model, x)\\n                loss = recon_loss + kl_loss1 + kl_loss2 + match_loss\\n                gradients = tape.gradient(loss, model.trainable_variables)\\n                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n                train_loss_avg.update_state(loss)\\n        # compute validation loss\\n        for x in val_dataset:\\n            recon_loss, kl_loss1, kl_loss2, match_loss = compute_loss(model, x)\\n            loss = recon_loss + kl_loss1 + kl_loss2 + match_loss\\n            val_loss_avg.update_state(loss)\\n\\n        train_loss_results.append(train_loss_avg.result())\\n        val_loss_results.append(val_loss_avg.result())\\n\\n        # print progress\\n        if epoch % 10 == 0:\\n            print(\\\"Epoch {:03d}: Train Loss: {:.3f}, Val Loss: {:.3f}\\\".format(epoch, train_loss_avg.result(), val_loss_avg.result()))\\n\\n        # check for early stopping\\n        if len(val_loss_results) > early_stopping.patience:\\n            if val_loss_results[-1] >= val_loss_results[-1 * (early_stopping.patience + 1)]:\\n                print(\\\"Early stopping at epoch {:03d}\\\".format(epoch))\\n                break\\n\\n    return train_loss_results, val_loss_results\";\n",
       "                var nbb_formatted_code = \"# define optimizer\\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\\n\\n# define early stopping\\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\\\"val_loss\\\", patience=5)\\n\\n# train the model\\ndef train(model, x_train, x_val, epochs, batch_size):\\n    train_dataset = (\\n        tf.data.Dataset.from_tensor_slices(x_train)\\n        .shuffle(len(x_train))\\n        .batch(batch_size)\\n    )\\n    val_dataset = tf.data.Dataset.from_tensor_slices(x_val).batch(batch_size)\\n\\n    train_loss_results = []\\n    val_loss_results = []\\n\\n    for epoch in range(epochs):\\n        train_loss_avg = tf.keras.metrics.Mean()\\n        val_loss_avg = tf.keras.metrics.Mean()\\n\\n        # train the model\\n        for x in train_dataset:\\n            with tf.GradientTape() as tape:\\n                recon_loss, kl_loss1, kl_loss2, match_loss = compute_loss(model, x)\\n                loss = recon_loss + kl_loss1 + kl_loss2 + match_loss\\n                gradients = tape.gradient(loss, model.trainable_variables)\\n                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n                train_loss_avg.update_state(loss)\\n        # compute validation loss\\n        for x in val_dataset:\\n            recon_loss, kl_loss1, kl_loss2, match_loss = compute_loss(model, x)\\n            loss = recon_loss + kl_loss1 + kl_loss2 + match_loss\\n            val_loss_avg.update_state(loss)\\n\\n        train_loss_results.append(train_loss_avg.result())\\n        val_loss_results.append(val_loss_avg.result())\\n\\n        # print progress\\n        if epoch % 10 == 0:\\n            print(\\n                \\\"Epoch {:03d}: Train Loss: {:.3f}, Val Loss: {:.3f}\\\".format(\\n                    epoch, train_loss_avg.result(), val_loss_avg.result()\\n                )\\n            )\\n\\n        # check for early stopping\\n        if len(val_loss_results) > early_stopping.patience:\\n            if (\\n                val_loss_results[-1]\\n                >= val_loss_results[-1 * (early_stopping.patience + 1)]\\n            ):\\n                print(\\\"Early stopping at epoch {:03d}\\\".format(epoch))\\n                break\\n\\n    return train_loss_results, val_loss_results\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# define early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# train the model\n",
    "def train(model, x_train, x_val, epochs, batch_size):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(len(x_train)).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(x_val).batch(batch_size)\n",
    "    \n",
    "    train_loss_results = []\n",
    "    val_loss_results = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss_avg = tf.keras.metrics.Mean()\n",
    "        val_loss_avg = tf.keras.metrics.Mean()\n",
    "        \n",
    "        # train the model\n",
    "        for x in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                recon_loss, kl_loss1, kl_loss2, match_loss = compute_loss(model, x)\n",
    "                loss = recon_loss + kl_loss1 + kl_loss2 + match_loss\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                train_loss_avg.update_state(loss)\n",
    "        # compute validation loss\n",
    "        for x in val_dataset:\n",
    "            recon_loss, kl_loss1, kl_loss2, match_loss = compute_loss(model, x)\n",
    "            loss = recon_loss + kl_loss1 + kl_loss2 + match_loss\n",
    "            val_loss_avg.update_state(loss)\n",
    "\n",
    "        train_loss_results.append(train_loss_avg.result())\n",
    "        val_loss_results.append(val_loss_avg.result())\n",
    "\n",
    "        # print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch {:03d}: Train Loss: {:.3f}, Val Loss: {:.3f}\".format(epoch, train_loss_avg.result(), val_loss_avg.result()))\n",
    "\n",
    "        # check for early stopping\n",
    "        if len(val_loss_results) > early_stopping.patience:\n",
    "            if val_loss_results[-1] >= val_loss_results[-1 * (early_stopping.patience + 1)]:\n",
    "                print(\"Early stopping at epoch {:03d}\".format(epoch))\n",
    "                break\n",
    "\n",
    "    return train_loss_results, val_loss_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d0ab0af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:39:13.583110Z",
     "start_time": "2023-04-10T08:39:09.554756Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"cvae\" \"                 f\"(type CVAE).\n\nInput 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 64), found shape=(500, 128)\n\nCall arguments received by layer \"cvae\" \"                 f\"(type CVAE):\n  • x=['tf.Tensor(shape=(500, 5000), dtype=float32)', 'tf.Tensor(shape=(500, 5000), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m cvae \u001b[38;5;241m=\u001b[39m CVAE(input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m, latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m train_loss_results, val_loss_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_rna_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_atac_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_rna_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_atac_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# plot loss curves\u001b[39;00m\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_loss_results, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [18], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, x_train, x_val, epochs, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 22\u001b[0m         recon_loss, kl_loss1, kl_loss2, match_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m         loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m kl_loss1 \u001b[38;5;241m+\u001b[39m kl_loss2 \u001b[38;5;241m+\u001b[39m match_loss\n\u001b[0;32m     24\u001b[0m         gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[1;32mIn [17], line 3\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, x, k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(model, x, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     x1, x2 \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m----> 3\u001b[0m     x_recon, mean1, logvar1, mean2, logvar2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# compute reconstruction loss\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     recon_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(x_recon \u001b[38;5;241m-\u001b[39m x1))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn [16], line 59\u001b[0m, in \u001b[0;36mCVAE.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m z \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([z1, z2], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# decode from the concatenated latent variable\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_recon, mean1, logvar1, mean2, logvar2\n",
      "Cell \u001b[1;32mIn [16], line 39\u001b[0m, in \u001b[0;36mCVAE.decode\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"cvae\" \"                 f\"(type CVAE).\n\nInput 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 64), found shape=(500, 128)\n\nCall arguments received by layer \"cvae\" \"                 f\"(type CVAE):\n  • x=['tf.Tensor(shape=(500, 5000), dtype=float32)', 'tf.Tensor(shape=(500, 5000), dtype=float32)']"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# encode new data into the latent space\\nx_rna_train = tf.convert_to_tensor(np.random.rand(500, 5000))\\nx_atac_train = tf.convert_to_tensor(np.random.rand(500, 5000))\\n\\nx_rna_val = tf.convert_to_tensor(np.random.rand(100, 5000))\\nx_atac_val = tf.convert_to_tensor(np.random.rand(100, 5000))\\n# train the model\\ncvae = CVAE(input_dim = 5000, latent_dim = 64, hidden_dim = 3)\\ntrain_loss_results, val_loss_results = train(cvae, [x_rna_train, x_atac_train], [x_rna_val, x_atac_val], epochs=100, batch_size=32)\\n\\n# plot loss curves\\nplt.plot(train_loss_results, label='Train Loss')\\nplt.plot(val_loss_results, label='Val Loss')\\nplt.legend()\\nplt.show()\";\n",
       "                var nbb_formatted_code = \"# encode new data into the latent space\\nx_rna_train = tf.convert_to_tensor(np.random.rand(500, 5000))\\nx_atac_train = tf.convert_to_tensor(np.random.rand(500, 5000))\\n\\nx_rna_val = tf.convert_to_tensor(np.random.rand(100, 5000))\\nx_atac_val = tf.convert_to_tensor(np.random.rand(100, 5000))\\n# train the model\\ncvae = CVAE(input_dim=5000, latent_dim=64, hidden_dim=3)\\ntrain_loss_results, val_loss_results = train(\\n    cvae,\\n    [x_rna_train, x_atac_train],\\n    [x_rna_val, x_atac_val],\\n    epochs=100,\\n    batch_size=32,\\n)\\n\\n# plot loss curves\\nplt.plot(train_loss_results, label=\\\"Train Loss\\\")\\nplt.plot(val_loss_results, label=\\\"Val Loss\\\")\\nplt.legend()\\nplt.show()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode new data into the latent space\n",
    "x_rna_train = tf.convert_to_tensor(np.random.rand(500, 5000))\n",
    "x_atac_train = tf.convert_to_tensor(np.random.rand(500, 5000))\n",
    "\n",
    "x_rna_val = tf.convert_to_tensor(np.random.rand(100, 5000))\n",
    "x_atac_val = tf.convert_to_tensor(np.random.rand(100, 5000))\n",
    "# train the model\n",
    "cvae = CVAE(input_dim=5000, latent_dim=64, hidden_dim=3)\n",
    "train_loss_results, val_loss_results = train(\n",
    "    cvae,\n",
    "    [x_rna_train, x_atac_train],\n",
    "    [x_rna_val, x_atac_val],\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "# plot loss curves\n",
    "plt.plot(train_loss_results, label=\"Train Loss\")\n",
    "plt.plot(val_loss_results, label=\"Val Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9bd45b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:08:37.244844Z",
     "start_time": "2023-04-03T08:08:37.238859Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(model, data):\n",
    "    \"\"\"\n",
    "    Encode data into the latent space using the trained model.\n",
    "    Args:\n",
    "        model (CoupledVAE): Trained coupled variational autoencoder model.\n",
    "        data (numpy.ndarray): Input data to be encoded.\n",
    "    Returns:\n",
    "        numpy.ndarray: Encoded latent space representation of the input data.\n",
    "    \"\"\"\n",
    "    # split the input data into RNA and ATAC data\n",
    "    x_rna, x_atac = data\n",
    "    # encode the RNA and ATAC data into the latent space\n",
    "    z_rna = model.encode(x_rna, modality='rna')\n",
    "    z_atac = model.encode(x_atac, modality='atac')\n",
    "    # concatenate the two latent space representations\n",
    "    z = tf.concat([z_rna, z_atac], axis=0)\n",
    "    return z.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode new data into the latent space\n",
    "x_rna_new = np.random.rand(100, 5000)\n",
    "x_atac_new = np.random.rand(80, 5000)\n",
    "z_new = encode(cvae, [x_rna_new, x_atac_new])\n",
    "\n",
    "# visualize the latent space\n",
    "colors = np.concatenate([np.ones((100,1)), np.zeros((80,1))], axis=0) # assign different colors to RNA and ATAC data\n",
    "plt.scatter(z_new[:,0], z_new[:,1], c=colors)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8195aa15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T08:08:44.566798Z",
     "start_time": "2023-04-03T08:08:44.549844Z"
    }
   },
   "outputs": [],
   "source": [
    "def match(model, data_atac):\n",
    "    \"\"\"\n",
    "    Match scRNA-seq cells to scATAC-seq cells using the trained model.\n",
    "    Args:\n",
    "        model (CoupledVAE): Trained coupled variational autoencoder model.\n",
    "        data_atac (numpy.ndarray): scATAC-seq data to match scRNA-seq cells to.\n",
    "    Returns:\n",
    "        numpy.ndarray: scRNA-seq data matched to the scATAC-seq cells.\n",
    "    \"\"\"\n",
    "    # encode the scATAC-seq data into the latent space\n",
    "    z_atac = model.encode(data_atac, modality='atac')\n",
    "    # initialize an empty list to store the matched scRNA-seq cells\n",
    "    data_rna_matched = []\n",
    "    # loop over each scATAC-seq cell\n",
    "    for i in range(z_atac.shape[0]):\n",
    "        # compute the Euclidean distance between the scATAC-seq cell and all scRNA-seq cells in the latent space\n",
    "        dist = tf.reduce_sum(tf.square(z_atac[i,:] - model.z_rna), axis=1)\n",
    "        # find the index of the closest scRNA-seq cell\n",
    "        index = tf.argmin(dist)\n",
    "        # append the matched scRNA-seq cell to the list\n",
    "        data_rna_matched.append(model.x_rna[index,:])\n",
    "    # convert the list to a numpy array\n",
    "    data_rna_matched = np.array(data_rna_matched)\n",
    "    return data_rna_matched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81922459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some new scATAC-seq data\n",
    "x_atac_new = np.random.rand(50, 5000)\n",
    "\n",
    "# match the new scATAC-seq data to scRNA-seq data\n",
    "x_rna_matched = match(cvae, x_atac_new)\n",
    "\n",
    "# compare the original scATAC-seq data to the matched scRNA-seq data\n",
    "for i in range(x_atac_new.shape[0]):\n",
    "    print(\"Original ATAC Cell {}: {}\".format(i, x_atac_new[i,:10]))\n",
    "    print(\"Matched RNA Cell {}: {}\".format(i, x_rna_matched[i,:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea560f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28119ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c525d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
